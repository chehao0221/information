import yfinance as yf
import requests
import datetime
import os
import feedparser
import urllib.parse

# =============================
# åŸºç¤è¨­å®š
# =============================
DISCORD_WEBHOOK_URL = os.getenv("NEWS_WEBHOOK_URL", "").strip()
CACHE_FILE = "data/sent_news.txt"
TZ_TW = datetime.timezone(datetime.timedelta(hours=8))
MAX_EMBEDS = 10
NEWS_HOURS_LIMIT = 12

PRICE_CACHE = {}

# =============================
# è‚¡åƒ¹èˆ‡æŒ‡æ•¸ç²å–ç³»çµ±
# =============================
def get_stock_price(sym):
    if sym in PRICE_CACHE:
        return PRICE_CACHE[sym]
    try:
        t = yf.Ticker(sym)
        # å„ªå…ˆä½¿ç”¨ fast_infoï¼Œè‹¥å¤±æ•ˆå‰‡å˜—è©¦åŸºç¤ info
        info = t.fast_info
        price = info.get("last_price") or t.info.get("regularMarketPrice")
        prev = info.get("previous_close") or t.info.get("regularMarketPreviousClose")
        
        if price and prev:
            pct = ((price - prev) / prev) * 100
            PRICE_CACHE[sym] = (price, pct)
            return price, pct
    except Exception:
        pass
    PRICE_CACHE[sym] = (None, None)
    return None, None

def get_market_price(market_type):
    try:
        sym = "^TWII" if market_type == "TW" else "^IXIC"
        name = "åŠ æ¬ŠæŒ‡æ•¸" if market_type == "TW" else "é‚£æ–¯é”å…‹"
        t = yf.Ticker(sym)
        info = t.fast_info
        cur = info.get("last_price") or t.info.get("regularMarketPrice")
        prev = info.get("previous_close") or t.info.get("regularMarketPreviousClose")
        
        if not cur or not prev: return "âš ï¸ è³‡æ–™è®€å–ä¸­"
        pct = ((cur - prev) / prev) * 100
        emoji = "ğŸ“ˆ" if pct > 0 else "ğŸ“‰" if pct < 0 else "â–"
        return f"{emoji} {name}: {cur:.2f} ({pct:+.2f}%)"
    except Exception:
        return "âš ï¸ æŒ‡æ•¸å–å¾—å¤±æ•—"

# =============================
# å€‹è‚¡å°ç…§è¡¨èˆ‡æ¬Šé‡åˆ¤å®š
# =============================
STOCK_MAP = {
    "å°ç©é›»": {"sym": "2330.TW", "desc": "AIæ™¶ç‰‡ / å…ˆé€²è£½ç¨‹"},
    "2330": {"sym": "2330.TW", "desc": "AIæ™¶ç‰‡ / å…ˆé€²è£½ç¨‹"},
    "é´»æµ·": {"sym": "2317.TW", "desc": "AIä¼ºæœå™¨ / çµ„è£"},
    "è¯ç™¼ç§‘": {"sym": "2454.TW", "desc": "ICè¨­è¨ˆ"},
    "å»£é”": {"sym": "2382.TW", "desc": "AIä¼ºæœå™¨ä»£å·¥"},
    "å¥‡é‹": {"sym": "3017.TW", "desc": "AIæ•£ç†±é¾é ­"},
    "é›™é´»": {"sym": "3324.TW", "desc": "æ¶²å†·æ•£ç†±"},
    "ä¸–èŠ¯": {"sym": "3661.TW", "desc": "ASIC è¨­è¨ˆ"},
    "é•·æ¦®": {"sym": "2603.TW", "desc": "èˆªé‹é¾é ­"},
    "00929": {"sym": "00929.TW", "desc": "ç§‘æŠ€å„ªæ¯"},
    "00919": {"sym": "00919.TW", "desc": "ç²¾é¸é«˜æ¯"},
    "è¼é”": {"sym": "NVDA", "desc": "NVIDIA / AIé¾é ­"},
    "NVIDIA": {"sym": "NVDA", "desc": "NVIDIA / AIé¾é ­"},
    "ç‰¹æ–¯æ‹‰": {"sym": "TSLA", "desc": "Tesla"},
    "TSLA": {"sym": "TSLA", "desc": "Tesla"},
    "è˜‹æœ": {"sym": "AAPL", "desc": "Apple"},
    "AAPL": {"sym": "AAPL", "desc": "Apple"},
    "å¾®è»Ÿ": {"sym": "MSFT", "desc": "Microsoft"},
    "ç¾è¶…å¾®": {"sym": "SMCI", "desc": "SMCI / ä¼ºæœå™¨"},
    "PLTR": {"sym": "PLTR", "desc": "AIæ•¸æ“šåˆ†æ"},
}

STOCK_WEIGHT = {
    "2330.TW": 5, "NVDA": 5, "AAPL": 4, "MSFT": 4, 
    "2454.TW": 4, "00929.TW": 4, "2317.TW": 3, "PLTR": 3,
}

def pick_most_important_stock(title):
    hits = []
    title_lower = title.lower()
    seen_sym = set()
    for key, info in STOCK_MAP.items():
        pos = title_lower.find(key.lower())
        if pos >= 0:
            sym = info["sym"]
            if sym in seen_sym: continue
            seen_sym.add(sym)
            weight = STOCK_WEIGHT.get(sym, 1)
            score = weight * 100 - pos
            hits.append((score, info))
    if not hits: return None
    hits.sort(reverse=True, key=lambda x: x[0])
    return hits[0][1]

# =============================
# Embed ç”Ÿæˆèˆ‡ä¸»æµç¨‹
# =============================
def create_news_embed(post, market_type):
    color = 0x3498db if market_type == "TW" else 0xe74c3c
    target = pick_most_important_stock(post["title"])

    if target:
        price, pct = get_stock_price(target["sym"])
        if price is not None:
            trend = "ğŸ“ˆ åˆ©å¤š" if pct > 0 else "ğŸ“‰ åˆ©ç©º" if pct < 0 else "â– ä¸­æ€§"
            return {
                "title": f"ğŸ“Š {target['sym']} | {target['desc']}",
                "url": post["link"],
                "color": color,
                "fields": [
                    {"name": "âš–ï¸ å¸‚å ´åˆ¤æ–·", "value": trend, "inline": True},
                    {"name": "ğŸ’µ å³æ™‚åƒ¹æ ¼", "value": f"**{price:.2f} ({pct:+.2f}%)**", "inline": True},
                    {"name": "ğŸ“° ç„¦é»æ–°è", "value": f"[{post['title']}]({post['link']})\nğŸ•’ {post['time']}", "inline": False},
                ],
                "footer": {"text": "Smart News Radar System"},
            }
    return {
        "title": post["title"],
        "url": post["link"],
        "color": color,
        "fields": [
            {"name": "âš–ï¸ å¸‚å ´åˆ¤æ–·", "value": "â– ä¸­æ€§", "inline": True},
            {"name": "ğŸ•’ ç™¼å¸ƒæ™‚é–“", "value": f"{post['time']} (å°åŒ—)", "inline": True},
            {"name": "ğŸ“° æ–°èä¾†æº", "value": post["source"], "inline": False},
        ],
        "footer": {"text": "Smart News Radar System"},
    }

def get_market_news(market_type):
    if not DISCORD_WEBHOOK_URL:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š Discord Webhook URL"); return

    os.makedirs("data", exist_ok=True)
    sent_titles = set()
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            sent_titles = {l.strip() for l in f if l.strip()}

    queries = (
        ["å°è‚¡ è²¡ç¶“", "å°ç©é›» é´»æµ· è¯ç™¼ç§‘", "00929 00919 é…æ¯"]
        if market_type == "TW"
        else ["ç¾è‚¡ ç›¤å‰", "è¼é” NVIDIA ç‰¹æ–¯æ‹‰", "PLTR SMCI è²¡å ±"]
    )

    label = "ğŸ¹ å°è‚¡å¸‚å ´å¿«è¨Š" if market_type == "TW" else "âš¡ ç¾è‚¡å¸‚å ´å¿«è¨Š"
    now_utc = datetime.datetime.now(datetime.timezone.utc)
    collected = {}

    for q in queries:
        url = f"https://news.google.com/rss/search?q={urllib.parse.quote(q)}&hl=zh-TW&gl=TW&ceid=TW:zh-TW"
        feed = feedparser.parse(url)
        for e in feed.entries[:10]:
            title = e.title.split(" - ")[0]
            if title in sent_titles or title in collected: continue
            if not hasattr(e, "published_parsed"): continue
            pub_utc = datetime.datetime(*e.published_parsed[:6], tzinfo=datetime.timezone.utc)
            if (now_utc - pub_utc).total_seconds() / 3600 > NEWS_HOURS_LIMIT: continue
            collected[title] = {
                "title": title, "link": e.link, "source": e.title.split(" - ")[-1],
                "time": pub_utc.astimezone(TZ_TW).strftime("%H:%M"), "sort": pub_utc,
            }

    posts = sorted(collected.values(), key=lambda x: x["sort"], reverse=True)[:MAX_EMBEDS]
    if not posts:
        print(f"â„¹ï¸ [{market_type}] ç›®å‰ç„¡æ–°æ–°è"); return

    embeds = [create_news_embed(p, market_type) for p in posts]
    payload = {
        "content": f"## {label}\nğŸ“… `{datetime.datetime.now(TZ_TW).strftime('%Y-%m-%d %H:%M')}`\nğŸ“Š **{get_market_price(market_type)}**\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
        "embeds": embeds,
    }

    r = requests.post(DISCORD_WEBHOOK_URL, json=payload, timeout=15)
    if r.status_code in (200, 204):
        sent_titles.update(p["title"] for p in posts)
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            for t in list(sent_titles)[-300:]: f.write(f"{t}\n")
        print(f"âœ… æˆåŠŸæ¨æ’­ {len(embeds)} å‰‡æ¶ˆæ¯")

if __name__ == "__main__":
    now_tw = datetime.datetime.now(TZ_TW)
    market = "TW" if 6 <= now_tw.hour < 17 else "US"
    print(f"ğŸ•’ å°åŒ—æ™‚é–“: {now_tw.strftime('%H:%M')}, å¸‚å ´æ¨¡å¼: {market}")
    get_market_news(market)
