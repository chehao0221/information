# -*- coding: utf-8 -*-
"""
Information push bot (TW / US / Crypto) -> Discord

é‡è¦ï¼šæ­¤ç‰ˆæœ¬æŠŠã€Œé¡¯ç¤ºæ–¹å¼ã€æ¢å¾©æˆä½ èˆŠåœ–çš„é¢¨æ ¼ï¼šä¸€å‰‡æ–°èä¸€å¼µ Embed å¡ç‰‡ï¼Œ
ä¸¦ä¾ã€Œé‡å¤§/ä¸­ç´š/ä¸€èˆ¬ã€å¥—ç”¨ç´…/é»ƒ/ç¶ é¡è‰²ã€‚

ç”±æ–¼ Google News RSS ä¸æä¾›å®Œæ•´æ¬„ä½ï¼ˆä¾†æº/æ™‚é–“/æ‘˜è¦ï¼‰ï¼Œæœ¬è…³æœ¬ç”¨ã€Œå¯è§£é‡‹ã€çš„æ–¹å¼è£œé½Šï¼š
- æ–°èä¾†æºï¼šå›ºå®šé¡¯ç¤º Google News
- ç™¼å¸ƒæ™‚é–“ï¼šä»¥æ¨æ’­æ™‚é–“ï¼ˆå°åŒ—ï¼‰é¡¯ç¤º
- å¸‚å ´åˆ¤æ–·/åˆ©å¤šåˆ©ç©ºï¼šç”¨æ¨™é¡Œé—œéµå­—ç°¡å–®åˆ†é¡ï¼ˆå¯è‡ªè¡Œèª¿æ•´ KEYWORDS_*ï¼‰
"""

import datetime
import os
import re
import urllib.parse
from typing import Dict, List, Tuple

import feedparser
import requests

# =========================
# åŸºæœ¬è¨­å®š
# =========================
DISCORD_WEBHOOK_URL = os.getenv("NEWS_WEBHOOK_URL", "").strip()
CACHE_FILE = "data/sent_news.txt"

GOOGLE_NEWS_RSS = "https://news.google.com/rss/search?q={query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

HTTP_TIMEOUT = int(os.getenv("HTTP_TIMEOUT", "12"))
MAX_ITEMS_PER_MARKET = int(os.getenv("MAX_ITEMS_PER_MARKET", "8"))  # æ¯å€‹å¸‚å ´æœ€å¤šå¹¾å‰‡æ–°è
# Discord é™åˆ¶ï¼šä¸€æ¬¡ webhook æœ€å¤š 10 embeds
MAX_EMBEDS_PER_REQUEST = 10

# =========================
# é‡è¦æ€§åˆ†ç´šï¼ˆå¯è‡ªè¡Œå¾®èª¿ï¼‰
# =========================
# é‡å¤§ï¼ˆç´…ï¼‰
KEYWORDS_MAJOR = [
    "æš´è·Œ", "å´©ç›¤", "ç†”æ–·", "ç·Šæ€¥", "é•ç´„", "ç ´ç”¢", "ä¸‹èª¿è©•ç´š", "è£å“¡", "åˆ¶è£",
    "å‡æ¯", "é™æ¯", "åˆ©ç‡æ±ºè­°", "FOMC", "CPI", "PCE", "NFP", "éè¾²",
    "åœ°ç·£", "æˆ°çˆ­", "è¡çª", "åœç«", "å°é–",
    "SEC", "è¨´è¨Ÿ", "åˆ¤æ±º", "èª¿æŸ¥",
    "ETFæ ¸å‡†", "ETFç²æ‰¹", "é§­å®¢", "è¢«ç›œ", "é»‘å®¢",
]
# ä¸­ç´šï¼ˆé»ƒï¼‰
KEYWORDS_MEDIUM = [
    "è²¡å ±", "å±•æœ›", "æŒ‡å¼•", "ç‡Ÿæ”¶", "æ¯›åˆ©", "EPS", "ç²åˆ©", "ä¸‹ä¿®", "ä¸Šä¿®",
    "ä½µè³¼", "æ”¶è³¼", "åˆä½œ", "æŠ•è³‡", "ç™¼è¡¨", "æ¨å‡º",
    "ç¾å…ƒ", "ç¾å‚µ", "æ®–åˆ©ç‡", "é€šè†¨", "æ²¹åƒ¹", "é‡‘åƒ¹",
    "æ¯”ç‰¹å¹£", "ä»¥å¤ªåŠ", "BTC", "ETH", "åŠ å¯†", "å¹£åœˆ",
]
# ä¸€èˆ¬ï¼ˆç¶ ï¼‰= å…¶ä»–

COLOR_RED = 0xE74C3C
COLOR_YELLOW = 0xF1C40F
COLOR_GREEN = 0x2ECC71

FOOTER_TEXT = "Smart News Radar System"


# =========================
# å·¥å…·å‡½å¼
# =========================
def _ensure_data_dir() -> None:
    os.makedirs(os.path.dirname(CACHE_FILE) or ".", exist_ok=True)


def _normalize_title(t: str) -> str:
    t = (t or "").strip()
    t = re.sub(r"\s+", " ", t)
    return t


def _load_sent_titles() -> List[str]:
    if not os.path.exists(CACHE_FILE):
        return []
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]


def _save_sent_titles(titles: List[str]) -> None:
    # å»é‡ + æ§åˆ¶å¤§å°ï¼ˆä¿ç•™æœ€æ–° 1500 ç­†ï¼‰
    seen = set()
    out: List[str] = []
    for t in titles:
        nt = _normalize_title(t)
        if not nt or nt in seen:
            continue
        seen.add(nt)
        out.append(nt)
    out = out[:1500]
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(out) + ("\n" if out else ""))


def _fetch_google_news(query: str) -> List[Dict[str, str]]:
    q = urllib.parse.quote_plus(query)
    url = GOOGLE_NEWS_RSS.format(query=q)
    feed = feedparser.parse(url)
    posts: List[Dict[str, str]] = []
    for e in (feed.entries or []):
        title = (e.get("title") or "").strip()
        link = (e.get("link") or "").strip()
        if not title or not link:
            continue
        posts.append({"title": title, "link": link})
    return posts


def _dedupe(posts: List[Dict[str, str]], sent_titles: List[str]) -> List[Dict[str, str]]:
    sent = set(_normalize_title(t) for t in sent_titles)
    out: List[Dict[str, str]] = []
    for p in posts:
        nt = _normalize_title(p.get("title", ""))
        if not nt or nt in sent:
            continue
        out.append(p)
    return out


def _classify_level(title: str) -> Tuple[str, int]:
    """å›å‚³ (ç­‰ç´šå­—ä¸², embed_color)"""
    t = title or ""
    for kw in KEYWORDS_MAJOR:
        if kw and kw in t:
            return "é‡å¤§", COLOR_RED
    for kw in KEYWORDS_MEDIUM:
        if kw and kw in t:
            return "ä¸­ç´š", COLOR_YELLOW
    return "ä¸€èˆ¬", COLOR_GREEN


def _extract_ticker_hint(title: str) -> str:
    """
    å˜—è©¦å¾æ¨™é¡ŒæŠ“å‡ºé¡ä¼¼ï¼š
    - 2330.TW
    - TSLA, AAPL
    - BTC, ETH
    å›å‚³ç”¨æ–¼å¡ç‰‡æ¨™é¡Œå‰ç¶´ï¼ŒæŠ“ä¸åˆ°å°±å›ç©ºå­—ä¸²ã€‚
    """
    if not title:
        return ""
    m = re.search(r"\b(\d{4}\.TW)\b", title)
    if m:
        return m.group(1)
    m = re.search(r"\b([A-Z]{2,6})\b", title)
    if m and m.group(1) not in {"OR", "AND", "THE"}:
        return m.group(1)
    return ""


def _build_header_embed(market_title: str, taipei_now: datetime.datetime) -> Dict:
    return {
        "title": market_title,
        "description": taipei_now.strftime("%Y-%m-%d %H:%Mï¼ˆå°åŒ—ï¼‰"),
        "color": 0x95A5A6,  # ç°è‰²åšç¸½æ¨™é¡Œ
        "footer": {"text": FOOTER_TEXT},
    }


def _build_news_embed(market: str, post: Dict[str, str], taipei_now: datetime.datetime) -> Dict:
    title = post["title"]
    url = post["link"]
    level, color = _classify_level(title)

    ticker = _extract_ticker_hint(title)
    card_title = f"{ticker} | {title}" if ticker else title
    if len(card_title) > 256:
        card_title = card_title[:253] + "..."

    fields = [
        {"name": "ğŸ·ï¸ ç­‰ç´š", "value": level, "inline": True},
        {"name": "ğŸ“Œ å¸‚å ´", "value": market, "inline": True},
        {"name": "ğŸ“° æ–°èä¾†æº", "value": "Google News", "inline": True},
        {"name": "ğŸ•’ ç™¼å¸ƒæ™‚é–“", "value": taipei_now.strftime("%H:%Mï¼ˆå°åŒ—ï¼‰"), "inline": True},
    ]

    # ä½ èˆŠåœ–æœ‰ã€Œå¸‚å ´åˆ¤æ–· / åˆ©å¤šã€ç­‰æ¬„ä½ï¼šé€™è£¡ç”¨ç°¡å–®å¯èª¿çš„è¦å‰‡å¡«å…¥
    # ï¼ˆä¹‹å¾Œä½ è¦å®Œå…¨å°é½ŠèˆŠå€‰åº«çš„è¦å‰‡ï¼Œå¯ä»¥æŠŠèˆŠå€‰åº«é‚£æ®µåˆ†é¡/æ‰“åˆ†é‚è¼¯è²¼éä¾†ï¼Œæˆ‘å†ç›´æ¥æ¬ï¼‰
    bias = "åˆ©å¤š" if level in ("é‡å¤§", "ä¸­ç´š") else "ä¸€èˆ¬"
    judge = "å¸‚å ´æ³¢å‹•" if level == "é‡å¤§" else ("é—œæ³¨äº‹ä»¶" if level == "ä¸­ç´š" else "ä¾‹è¡Œæ›´æ–°")
    fields.extend(
        [
            {"name": "âš–ï¸ å¸‚å ´åˆ¤æ–·", "value": judge, "inline": True},
            {"name": "ğŸ“ˆ åˆ©å¤š/åˆ©ç©º", "value": bias, "inline": True},
        ]
    )

    return {
        "title": card_title,
        "url": url,
        "color": color,
        "fields": fields,
        "footer": {"text": FOOTER_TEXT},
    }


def _post_webhook(payload: Dict) -> None:
    if not DISCORD_WEBHOOK_URL:
        print("âš ï¸ NEWS_WEBHOOK_URL æœªè¨­å®šï¼Œè·³éæ¨æ’­ã€‚")
        return
    r = requests.post(DISCORD_WEBHOOK_URL, json=payload, timeout=HTTP_TIMEOUT)
    if r.status_code >= 300:
        raise RuntimeError(f"Discord webhook failed: {r.status_code} {r.text[:500]}")


def send_embeds_in_batches(embeds: List[Dict]) -> None:
    """
    Discord webhookï¼šä¸€æ¬¡æœ€å¤š 10 embeds
    """
    if not embeds:
        return
    batch: List[Dict] = []
    for e in embeds:
        batch.append(e)
        if len(batch) >= MAX_EMBEDS_PER_REQUEST:
            _post_webhook({"embeds": batch})
            batch = []
    if batch:
        _post_webhook({"embeds": batch})


# =========================
# ä¸»æµç¨‹
# =========================
def run_push(label: str) -> None:
    """
    ä»¥ã€Œå°è‚¡ / ç¾è‚¡ / Cryptoã€ç‚ºä¸»ã€‚
    é¡¯ç¤ºæ–¹å¼ï¼šæ¯å€‹å¸‚å ´å…ˆé€ä¸€å¼µç¸½æ¨™é¡Œå¡ï¼Œå†ã€Œæ¯å‰‡æ–°èä¸€å¼µå¡ã€ã€‚
    """
    _ensure_data_dir()
    sent_titles = _load_sent_titles()

    # ä¸»è¦é—œéµå­—ï¼ˆä½ å¯ä»¥ä¹‹å¾Œå†è‡ªè¡Œå¾®èª¿ï¼‰
    tw_query = "å°è‚¡ OR å°ç£ è‚¡å¸‚ OR åŠ æ¬ŠæŒ‡æ•¸ OR å°æŒ‡æœŸ OR å°ç©é›»"
    us_query = "ç¾è‚¡ OR ç¾åœ‹ è‚¡å¸‚ OR é“ç“Š OR é‚£æ–¯é”å…‹ OR æ¨™æ™®500 OR è¯æº–æœƒ OR Fed"
    crypto_query = "æ¯”ç‰¹å¹£ OR ä»¥å¤ªåŠ OR åŠ å¯†è²¨å¹£ OR Bitcoin OR Ethereum"

    tw_posts = _dedupe(_fetch_google_news(tw_query), sent_titles)[:MAX_ITEMS_PER_MARKET]
    us_posts = _dedupe(_fetch_google_news(us_query), sent_titles)[:MAX_ITEMS_PER_MARKET]
    crypto_posts = _dedupe(_fetch_google_news(crypto_query), sent_titles)[:MAX_ITEMS_PER_MARKET]

    if not (tw_posts or us_posts or crypto_posts):
        print("âœ… ç„¡æ–°å…§å®¹ï¼ˆå¯èƒ½éƒ½å·²æ¨æ’­éï¼‰ï¼Œè·³éã€‚")
        return

    taipei_tz = datetime.timezone(datetime.timedelta(hours=8))
    now = datetime.datetime.now(taipei_tz)

    embeds: List[Dict] = []

    if tw_posts:
        embeds.append(_build_header_embed("ğŸ¹ å°è‚¡å¸‚å ´å¿«è¨Š", now))
        embeds.extend([_build_news_embed("å°è‚¡", p, now) for p in tw_posts])

    if us_posts:
        embeds.append(_build_header_embed("âš¡ ç¾è‚¡å¸‚å ´å¿«è¨Š", now))
        embeds.extend([_build_news_embed("ç¾è‚¡", p, now) for p in us_posts])

    if crypto_posts:
        embeds.append(_build_header_embed("ğŸª™ Crypto å¸‚å ´å¿«è¨Š", now))
        embeds.extend([_build_news_embed("Crypto", p, now) for p in crypto_posts])

    # é€å‡ºï¼ˆåˆ†æ‰¹ï¼‰
    send_embeds_in_batches(embeds)

    # æ›´æ–°å¿«å–ï¼šæŠŠæœ¬æ¬¡æ–°æ¨æ’­çš„ title åŠ å…¥ï¼ˆæ”¾å‰é¢ï¼Œé¿å…é‡è¤‡ï¼‰
    new_titles = [p["title"] for p in (tw_posts + us_posts + crypto_posts)]
    _save_sent_titles(new_titles + sent_titles)


def _label_by_time(taipei_now: datetime.datetime) -> str:
    """
    ä¿ç•™ä½ åŸæœ¬çš„æ™‚æ®µæ¨™ç±¤ï¼ˆworkflow åªæ˜¯ç”¨é€™å€‹åšæ¨™é¡Œ/è¾¨è­˜ï¼‰
    """
    h = taipei_now.hour
    m = taipei_now.minute

    # 08:30 å·¦å³
    if h == 8 and 0 <= m <= 59:
        return "ğŸ¹ å°è‚¡å¸‚å ´å¿«è¨Š"
    # 13:30 å·¦å³
    if h == 13 and 0 <= m <= 59:
        return "ğŸ¹ å°è‚¡åˆç›¤å¿«è¨Š"
    # 21:30 å·¦å³
    if h == 21 and 0 <= m <= 59:
        return "âš¡ ç¾è‚¡ç›¤å‰å¿«è¨Š"
    # 06:00 å·¦å³
    if h == 6 and 0 <= m <= 59:
        return "ğŸŒ™ ç¾è‚¡ç›¤å¾Œå›é¡§"

    # fallbackï¼šæ‰‹å‹•è§¸ç™¼æˆ–ä¸åœ¨æ’ç¨‹æ™‚æ®µ
    if 8 <= h < 17:
        return "ğŸ¹ å°è‚¡å¿«è¨Š"
    return "âš¡ ç¾è‚¡å¿«è¨Š"


if __name__ == "__main__":
    taipei_tz = datetime.timezone(datetime.timedelta(hours=8))
    now = datetime.datetime.now(taipei_tz)
    label = _label_by_time(now)
    run_push(label)
