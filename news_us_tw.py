import yfinance as yf
import requests
import datetime
import os
import feedparser
import urllib.parse

# =============================
# åŸºç¤è¨­å®š
# =============================
DISCORD_WEBHOOK_URL = os.getenv("NEWS_WEBHOOK_URL", "").strip()
CACHE_FILE = "data/sent_news.txt"
TZ_TW = datetime.timezone(datetime.timedelta(hours=8))
MAX_EMBEDS = 10
NEWS_HOURS_LIMIT = 12

# =============================
# è‚¡åƒ¹å¿«å– (æ¸›å°‘ API è«‹æ±‚)
# =============================
PRICE_CACHE = {}

def get_stock_price(sym):
    if sym in PRICE_CACHE:
        return PRICE_CACHE[sym]
    try:
        t = yf.Ticker(sym)
        info = t.fast_info
        price = info.get("last_price")
        prev = info.get("previous_close")
        if price and prev:
            pct = ((price - prev) / prev) * 100
            PRICE_CACHE[sym] = (price, pct)
            return price, pct
    except Exception:
        pass
    PRICE_CACHE[sym] = (None, None)
    return None, None

# =============================
# å€‹è‚¡ / ETF é—œéµå­—è¡¨ï¼ˆæ“´å……æ¨è–¦å€‹è‚¡ï¼‰
# =============================
STOCK_MAP = {
    # --- å°è‚¡é‡é»æ¬Šå€¼ ---
    "å°ç©é›»": {"sym": "2330.TW", "desc": "AIæ™¶ç‰‡ / å…ˆé€²è£½ç¨‹"},
    "é´»æµ·": {"sym": "2317.TW", "desc": "AIä¼ºæœå™¨ / çµ„è£"},
    "è¯ç™¼ç§‘": {"sym": "2454.TW", "desc": "ICè¨­è¨ˆ"},
    "å»£é”": {"sym": "2382.TW", "desc": "AIä¼ºæœå™¨ä»£å·¥"},
    
    # --- å°è‚¡ç†±é–€æ—ç¾¤ (æ¨è–¦å¢åŠ ) ---
    "å¥‡é‹": {"sym": "3017.TW", "desc": "AIæ•£ç†±é¾é ­"},
    "é›™é´»": {"sym": "3324.TW", "desc": "æ¶²å†·æ•£ç†±æŠ€è¡“"},
    "ä¸–èŠ¯": {"sym": "3661.TW", "desc": "ASIC è¨­è¨ˆ"},
    "é•·æ¦®": {"sym": "2603.TW", "desc": "èˆªé‹é¾é ­"},
    "é™½æ˜": {"sym": "2609.TW", "desc": "æµ·é‹å¸‚å ´"},
    
    # --- å°è‚¡é‡‘è / ETF ---
    "å¯Œé‚¦é‡‘": {"sym": "2881.TW", "desc": "é‡‘èé¾é ­"},
    "åœ‹æ³°é‡‘": {"sym": "2882.TW", "desc": "é‡‘èæ§è‚¡"},
    "0050": {"sym": "0050.TW", "desc": "å°ç£50 ETF"},
    "00878": {"sym": "00878.TW", "desc": "åœ‹æ³°æ°¸çºŒé«˜è‚¡æ¯"},
    "00929": {"sym": "00929.TW", "desc": "å¾©è¯ç§‘æŠ€å„ªæ¯"},
    "00940": {"sym": "00940.TW", "desc": "å…ƒå¤§å°ç£åƒ¹å€¼é«˜æ¯"},

    # --- ç¾è‚¡ç§‘æŠ€å·¨é ­ ---
    "è¼é”": {"sym": "NVDA", "desc": "NVIDIA / AIé¾é ­"},
    "NVIDIA": {"sym": "NVDA", "desc": "NVIDIA / AIé¾é ­"},
    "ç‰¹æ–¯æ‹‰": {"sym": "TSLA", "desc": "Tesla / é›»å‹•è»Š"},
    "è˜‹æœ": {"sym": "AAPL", "desc": "Apple"},
    "å¾®è»Ÿ": {"sym": "MSFT", "desc": "Microsoft / AIé›²ç«¯"},
    "Google": {"sym": "GOOGL", "desc": "Alphabet / AIæœå°‹"},
    "ç¾è¶…å¾®": {"sym": "SMCI", "desc": "SMCI / ä¼ºæœå™¨"},
    "Palantir": {"sym": "PLTR", "desc": "AIæ•¸æ“šåˆ†æ"},

    # --- ç¾è‚¡ ETF ---
    "QQQ": {"sym": "QQQ", "desc": "é‚£æ–¯é”å…‹ 100 ETF"},
    "SOXX": {"sym": "SOXX", "desc": "åŠå°é«” ETF"},
}

# =============================
# æŒ‡æ•¸æ‘˜è¦
# =============================
def get_market_price(market_type):
    try:
        sym = "^TWII" if market_type == "TW" else "^IXIC"
        name = "åŠ æ¬ŠæŒ‡æ•¸" if market_type == "TW" else "é‚£æ–¯é”å…‹"
        t = yf.Ticker(sym)
        info = t.fast_info
        cur = info.get("last_price")
        prev = info.get("previous_close")
        if not cur or not prev: return "âš ï¸ æŒ‡æ•¸è³‡æ–™ä¸è¶³"
        pct = ((cur - prev) / prev) * 100
        emoji = "ğŸ“ˆ" if pct > 0 else "ğŸ“‰" if pct < 0 else "â–"
        return f"{emoji} {name}: {cur:.2f} ({pct:+.2f}%)"
    except Exception:
        return "âš ï¸ æŒ‡æ•¸å–å¾—å¤±æ•—"

# =============================
# Embed ç”Ÿæˆ (ç¶­æŒæ‚¨å–œæ­¡çš„å¥½çœ‹æ’ç‰ˆ)
# =============================
def create_news_embed(post, market_type):
    color = 0x3498db if market_type == "TW" else 0xe74c3c

    for key, info in STOCK_MAP.items():
        if key in post["title"]:
            price, pct = get_stock_price(info["sym"])
            if price is not None:
                trend = "ğŸ“ˆ åˆ©å¤š" if pct > 0 else "ğŸ“‰ åˆ©ç©º" if pct < 0 else "â– ä¸­æ€§"
                return {
                    "title": f"ğŸ“Š {info['sym']} | {info['desc']}",
                    "url": post["link"],
                    "color": color,
                    "fields": [
                        {"name": "âš–ï¸ å¸‚å ´åˆ¤æ–·", "value": trend, "inline": True},
                        {"name": "ğŸ’µ å³æ™‚åƒ¹æ ¼", "value": f"**{price:.2f} ({pct:+.2f}%)**", "inline": True},
                        {"name": "ğŸ“° ç„¦é»æ–°è", "value": f"[{post['title']}]({post['link']})\nğŸ•’ {post['time']}", "inline": False},
                    ],
                    "footer": {"text": "Quant Bot Intelligence System"},
                }

    return {
        "title": post["title"],
        "url": post["link"],
        "color": color,
        "fields": [
            {"name": "âš–ï¸ å¸‚å ´åˆ¤æ–·", "value": "â– ä¸­æ€§", "inline": True},
            {"name": "ğŸ•’ ç™¼å¸ƒæ™‚é–“", "value": f"{post['time']} (å°åŒ—)", "inline": True},
            {"name": "ğŸ“° æ–°èä¾†æº", "value": post["source"], "inline": False},
        ],
        "footer": {"text": "Quant Bot Intelligence System"},
    }

# =============================
# ä¸»æµç¨‹ (å«é˜²é‡è¤‡æ©Ÿåˆ¶)
# =============================
def get_market_news(market_type):
    if not DISCORD_WEBHOOK_URL:
        print("âŒ æœªè¨­å®š Webhook URL"); return

    os.makedirs("data", exist_ok=True)
    sent = set()
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            sent = {l.strip() for l in f if l.strip()}

    # æ“´å±•æœå°‹é—œéµå­—ä»¥å¢åŠ å€‹è‚¡æ–°èå‘½ä¸­ç‡
    if market_type == "TW":
        queries = ["å°è‚¡ è²¡ç¶“", "å°ç©é›» é´»æµ· è¯ç™¼ç§‘", "æ•£ç†± å¥‡é‹ é›™é´»", "ETF é…æ¯ 00929"]
    else:
        queries = ["ç¾è‚¡ ç›¤å‰", "è¼é” NVIDIA ç‰¹æ–¯æ‹‰", "AI è‚¡ç¥¨ è²¡å ±", "PLTR SMCI èµ°å‹¢"]

    label = "ğŸ¹ å°è‚¡å¸‚å ´å¿«è¨Š | Morning Brief" if market_type == "TW" else "âš¡ ç¾è‚¡å¸‚å ´å¿«è¨Š | Market Brief"
    now_utc = datetime.datetime.now(datetime.timezone.utc)
    collected = {}

    for q in queries:
        feed = feedparser.parse(f"https://news.google.com/rss/search?q={urllib.parse.quote(q)}&hl=zh-TW&gl=TW&ceid=TW:zh-TW")
        for e in feed.entries[:10]: # å¢åŠ å–®å€‹æœå°‹çš„æƒæé‡
            title = e.title.split(" - ")[0]
            # --- é˜²é‡è¤‡åˆ¤æ–· ---
            if title in sent or title in collected or not hasattr(e, "published_parsed"):
                continue
            pub_utc = datetime.datetime(*e.published_parsed[:6], tzinfo=datetime.timezone.utc)
            if (now_utc - pub_utc).total_seconds() / 3600 > NEWS_HOURS_LIMIT:
                continue
            
            collected[title] = {
                "title": title, "link": e.link,
                "source": e.title.split(" - ")[-1],
                "time": pub_utc.astimezone(TZ_TW).strftime("%H:%M"),
                "sort": pub_utc,
            }

    posts = sorted(collected.values(), key=lambda x: x["sort"], reverse=True)[:MAX_EMBEDS]
    if not posts: return
    
    embeds = [create_news_embed(p, market_type) for p in posts]

    payload = {
        "content": f"## {label}\nğŸ“… `{datetime.datetime.now(TZ_TW).strftime('%Y-%m-%d %H:%M')}`\nğŸ“Š **{get_market_price(market_type)}**\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
        "embeds": embeds,
    }

    r = requests.post(DISCORD_WEBHOOK_URL, json=payload, timeout=15)
    if r.status_code in (200, 204):
        sent.update(p["title"] for p in posts)
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            for t in list(sent)[-300:]: f.write(f"{t}\n")
        print(f"âœ… æ¨é€æˆåŠŸ {len(embeds)} å‰‡")

if __name__ == "__main__":
    now = datetime.datetime.now(TZ_TW)
    # åˆ¤æ–·æ™‚æ®µåˆ‡æ›å¸‚å ´ï¼š06:00~17:00 è·‘å°è‚¡ï¼Œå…¶é¤˜æ™‚é–“è·‘ç¾è‚¡
    market = "TW" if 6 <= now.hour < 17 else "US"
    get_market_news(market)
